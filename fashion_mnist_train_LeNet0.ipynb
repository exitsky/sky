{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_mnist_train_LeNet0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/exitsky/sky/blob/master/fashion_mnist_train_LeNet0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "7YI32Lk--hKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len - 1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
        "    return y[5:len(y) - 5]\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "\n",
        "    npad = [(0, 0), (0, 0), (pad, pad), (pad, pad)]\n",
        "    img = np.pad(input_data, npad, 'constant')\n",
        "\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride * out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride * out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
        "    \n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride * out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride * out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "  \n",
        "  \n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x >= 0] = 1\n",
        "    return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2: \n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t) ** 2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(x, t):\n",
        "    y = softmax(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.original_x_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "\n",
        "        if x.ndim == 1:\n",
        "            x = x.reshape(1, x.size)\n",
        "            \n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "    \n",
        "class BatchNormalization:\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None\n",
        "\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.xn = None\n",
        "        self.std = None\n",
        "\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flag=True):\n",
        "        self.input_shape = x.shape\n",
        "\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "#             print(\"BatchNormalization forward -> N: {}, C: {}, H: {}, W: {}\".format(N, C, H, W))\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flag)\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flag):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flag:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc ** 2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / (np.sqrt(self.running_var + 10e-7))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2 :\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis = 0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "\n",
        "        dmu = -np.sum(dxc, axis=0)\n",
        "        dx = dxc + dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            \n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        \n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "\n",
        "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
        "\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "  \n",
        "  \n",
        "  \n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        print(self.lr, self.momentum)\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / ((np.sqrt(self.h[key])) + 1e-8)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "    def __init__(self, lr=0.01, decay_rate=0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            # 분산 mean square\n",
        "            # h <- ah + (1 - a) * (dw * dw)\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            # 표준편차 root mean square : 1/sqrt(h)\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-8)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.t = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.t += 1\n",
        "        alpha = self.lr * np.sqrt(1.0 - self.beta2 ** self.t) / (1.0 - self.beta1 ** self.t)\n",
        "\n",
        "        for key in params.keys() :\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])\n",
        "\n",
        "            params[key] -= alpha * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "212DdXiy-k77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test\n",
        "                 , epochs=20\n",
        "                 , mini_batch_size=100\n",
        "                 , optimizer='sgd'\n",
        "                 , optimizer_param={'lr': 0.01}\n",
        "                 , evaluate_sample_num_per_epoch=None\n",
        "                 , verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        optimizer_class_dict = {'sgd': SGD, 'momentum': Momentum, 'nesterov': Nesterov, 'adagrad': AdaGrad, 'rmsprpo': RMSprop, 'adam': Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "#         if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            \n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\n",
        "                \"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rd7mP5ZO-60L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class LeNet:\n",
        "    \"\"\"\n",
        "    네트워크 구성은 아래와 같음\n",
        "        conv - relu - pool-\n",
        "        conv - relu - pool -\n",
        "        affine - relu - \n",
        "        affine - softmax\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1={'filter_num': 6, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                 conv_param_2={'filter_num': 16, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                 hidden_size_list=[100],\n",
        "                 output_size=10,\n",
        "                 weight_init_std=0.01):\n",
        "        self.conv_layer_num = len([conv_param_1, conv_param_2])\n",
        "        self.affine_layer_size_list = hidden_size_list + [output_size]\n",
        "        self.conv_affine_layer_num = self.conv_layer_num + len(self.affine_layer_size_list)\n",
        "     \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "\n",
        "        # < 가중치 초기화 >    \n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2]):\n",
        "            self.params['W' + str(idx + 1)] = weight_init_std * np.random.randn(conv_param['filter_num'],\n",
        "                                                                                       pre_channel_num,\n",
        "                                                                                       conv_param['filter_size'],\n",
        "                                                                                       conv_param['filter_size'])\n",
        "            self.params['b' + str(idx + 1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(16 * 4 * 4, hidden_size_list[0])\n",
        "        self.params['b3'] = np.zeros(hidden_size_list[0])\n",
        "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size_list[0], output_size)\n",
        "        self.params['b4'] = np.zeros(output_size)\n",
        "\n",
        "    \n",
        "        # < 계층 생성 >\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
        "        self.layers['Relu3'] = Relu()\n",
        "        self.layers['Affine4'] = Affine(self.params['W4'], self.params['b4'])\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "        \n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for key, layer in self.layers.items():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i * batch_size:(i + 1) * batch_size]\n",
        "            tt = t[i * batch_size:(i + 1) * batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return (acc / x.shape[0]) * 100     # 백분률 (%)\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 기울기 계산 결과\n",
        "        grads = {}\n",
        "        for idx in range(1, self.conv_layer_num + 1):\n",
        "            grads['W' + str(idx)] = self.layers['Conv' + str(idx)].dW\n",
        "            grads['b' + str(idx)] = self.layers['Conv' + str(idx)].db\n",
        "\n",
        "        for idx in range(self.conv_layer_num + 1, self.conv_affine_layer_num + 1):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z6H2veSy--Vy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "580de426-b3b5-4426-ad7e-aa092a333b5f"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# Download the fashion_mnist data\n",
        "##############################################################################\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "###################################################################\n",
        "# Fashion Mnist 데이터가 기존 Mnist와 데이터 포맷이 달라서 코드가 호환이 안된다.\n",
        "# 아래 코드를 사용해서 변환하면된다\n",
        "###################################################################\n",
        "x_train=np.expand_dims(x_train,axis=1)\n",
        "x_test=np.expand_dims(x_test,axis=1)\n",
        "\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "# x_train, t_train = x_train[:10000], t_train[:10000]\n",
        "# x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "network = LeNet()\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test\n",
        "                  , epochs=15\n",
        "                  , mini_batch_size=100 \n",
        "                  , optimizer='Adam'\n",
        "                  , optimizer_param={'lr':0.001}\n",
        "                  , evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=5)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=5)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 100.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== epoch:1, train acc:21.3, test acc:21.9 ===\n",
            "=== epoch:2, train acc:84.6, test acc:85.0 ===\n",
            "=== epoch:3, train acc:89.1, test acc:86.2 ===\n",
            "=== epoch:4, train acc:90.6, test acc:87.9 ===\n",
            "=== epoch:5, train acc:91.3, test acc:88.6 ===\n",
            "=== epoch:6, train acc:90.2, test acc:87.0 ===\n",
            "=== epoch:7, train acc:92.1, test acc:88.2 ===\n",
            "=== epoch:8, train acc:92.5, test acc:87.4 ===\n",
            "=== epoch:9, train acc:92.2, test acc:89.0 ===\n",
            "=== epoch:10, train acc:91.5, test acc:88.2 ===\n",
            "=== epoch:11, train acc:93.2, test acc:89.2 ===\n",
            "=== epoch:12, train acc:93.1, test acc:88.4 ===\n",
            "=== epoch:13, train acc:94.1, test acc:89.6 ===\n",
            "=== epoch:14, train acc:93.5, test acc:89.6 ===\n",
            "=== epoch:15, train acc:93.9, test acc:88.6 ===\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:89.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFcCAYAAAAzhzxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fee656ac950>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}